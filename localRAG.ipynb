{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers faiss-cpu torch\n",
    "# !pip install pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import pdfplumber\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACHMENT_TYPE_ERR_MSG = (\n",
    "    \"All attachments must be either DctmObjRef or Attachment type, got {}: {}\"\n",
    ")\n",
    "ATTACH_TYPE_EXPECTED = \"Attachment expected to be of type `Attachment`, got {}\"\n",
    "UNEXPECTED_ATTR_TO_PARSE = (\n",
    "    \"Attribute to parse from attachments expected to be in \"\n",
    "    \"['body', 'filename'], got '{}'\"\n",
    ")\n",
    "DCTM_OBJ_REF_EXPECTED = \"Expected DctmObjRef, got {}: {}\"\n",
    "DOXC2TXT_EXCEPTION = \"Cannot process file, raised '{}' error\"\n",
    "LIST_OR_STR_ATTACH_EXPECTED = \"Got type {} for attachment, only list or str accepted\"\n",
    "PAGE_SEP = \"\\n\" + \"=\" * 31 + \" NEW PAGE \" + \"=\" * 31 + \"\\n\"\n",
    "MISSING_SPACES_PATTERNS = [\n",
    "    \"IndicativeTermsheet\\n\",\n",
    "    \"PRIVATEPLACEMENT\\n\",\n",
    "    \"PublicOfferingonlyin:\",\n",
    "]\n",
    "\n",
    "\n",
    "def check_txt_missing_spaces(all_pages_txt: str, threshold: float = 0.06) -> bool:\n",
    "    \"\"\"Check if the parsed PDF has missing spaces (as for all Leonteq termsheets).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The alignment used to format the Leonteq termsheets are not properly recognized by\n",
    "    our PDF converter. As an undesirable result, most spaces are being removed during\n",
    "    the conversion step leading to erroneous extractions.\n",
    "    \"\"\"\n",
    "\n",
    "    nb_spaces = all_pages_txt.count(\" \")\n",
    "    nb_chars = len(all_pages_txt)\n",
    "    ratio = nb_spaces / nb_chars\n",
    "\n",
    "    return ratio < threshold and any(\n",
    "        p in all_pages_txt for p in MISSING_SPACES_PATTERNS\n",
    "    )\n",
    "\n",
    "\n",
    "def pdf_text_from_bytes(\n",
    "    pdf_bytes_string: bytes,\n",
    "    max_pages: int = 999,\n",
    "    pages_sep: str = PAGE_SEP,\n",
    ") -> str:\n",
    "    \"\"\"Convert the PDF byte representation to text.\"\"\"\n",
    "    try:\n",
    "        # Pdfplumber returns empty string for UTF-8 encoded strings\n",
    "        # (without any exception raised), only Latin-1 works\n",
    "        # On the other hand, FastAPI requires UTF-8 strings in payloads,\n",
    "        # so we assume UTF-8 string arrives here\n",
    "        pdf_bytes_string = pdf_bytes_string.decode(\"UTF-8\").encode(\"Latin1\")\n",
    "    except UnicodeDecodeError:\n",
    "        # If the above command fails, we will assume the byte string\n",
    "        # is already Latin1 encoded\n",
    "        pass\n",
    "\n",
    "    all_pages_txt = \"\"\n",
    "    pages_list = []\n",
    "    with pdfplumber.open(io.BytesIO(pdf_bytes_string)) as pdf:\n",
    "        for page_idx in range(\n",
    "            0, min(len(pdf.pages), max_pages)\n",
    "        ):  # pylint: disable=invalid-name\n",
    "            pages_list.append(pdf.pages[page_idx].extract_text() + pages_sep)\n",
    "            all_pages_txt += pdf.pages[page_idx].extract_text() + pages_sep\n",
    "    return pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"data/llama2.pdf\",\n",
    "    \"rb\",\n",
    ") as fobj:\n",
    "    pdf_bytes_utf8 = fobj.read().decode(\"Latin1\").encode(\"UTF-8\")\n",
    "    pdf_text = pdf_text_from_bytes(pdf_bytes_utf8)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "cleaned_txt = []\n",
    "for page in pdf_text:\n",
    "    cleaned_txt.append(clean_text(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_txt = cleaned_txt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_keys(filename):\n",
    "    mykey = None\n",
    "    myorg = None\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"r\") as file:\n",
    "            for line in file:\n",
    "                if \"hugging_face_key\" in line:\n",
    "                    mykey = line.split(\"=\")[1].strip().strip('\"')\n",
    "                elif \"myorg\" in line:\n",
    "                    myorg = line.split(\"=\")[1].strip().strip('\"')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filename} does not exist.\")\n",
    "\n",
    "    return mykey, myorg\n",
    "\n",
    "\n",
    "# Reading values from keys.txt\n",
    "filename = \"/Users/naimsassine/Desktop/DSAI/keys\"\n",
    "mykey, myorg = read_keys(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def embed_text(text, tokenizer, model):\n",
    "    # Tokenize and get model embeddings\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)  # mean pooling\n",
    "    return embeddings.numpy()\n",
    "\n",
    "\n",
    "# Assume `pdf_pages` is a list of strings where each string is the text of one page of your PDF\n",
    "pdf_pages = cleaned_txt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to get embeddings using Hugging Face model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Index documents\n",
    "\n",
    "st = \"testing this phrase\"\n",
    "embedding = get_embeddings(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Embed all pages and store them in a list\n",
    "page_embeddings = [embed_text(page, tokenizer, model) for page in pdf_pages]\n",
    "page_embeddings = np.vstack(page_embeddings)  # Stack embeddings into a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize embeddings for cosine similarity\n",
    "normalized_embeddings = normalize(page_embeddings)\n",
    "\n",
    "# Step 3: Create a FAISS index and add embeddings\n",
    "dimension = normalized_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(\n",
    "    dimension\n",
    ")  # IP for inner product, since embeddings are normalized\n",
    "index.add(normalized_embeddings)\n",
    "\n",
    "\n",
    "# Function to perform similarity search\n",
    "def search_similar_pages(query, index, tokenizer, model, k=3):\n",
    "    query_embedding = embed_text(query, tokenizer, model)\n",
    "    query_embedding = normalize(query_embedding)  # Normalize query embedding\n",
    "    _, top_k_indices = index.search(query_embedding, k)\n",
    "    return top_k_indices[0]\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"Some search query related to the content of the PDF\"\n",
    "top_k_pages = search_similar_pages(query, index, tokenizer, model, k=3)\n",
    "\n",
    "# Output relevant pages\n",
    "for i in top_k_pages:\n",
    "    print(f\"Relevant page {i+1}: {pdf_pages[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
